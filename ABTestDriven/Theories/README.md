# AB 实验原理



## 前提

* 同一时间维度（同时性）
时间的统一性有效的规避了因为时间、季节等因素带来的影响

* 相似属性分组用户（同质性）
两个策略对应的使用群体需要保证尽量一致。
属性的相似性则使得地域、性别、年龄等等其他因素对效果统计的影响降至最低。

## 不适用
尽管A/B Test可以弥补产品优化中遇到的不足，但它并不完全适用于所有的产品。
因为A/B Test的结果需要大量数据支撑，日流量越大的网站得出结果越准确。
通常来说，我们建议在进行A/B Test时，能够保证 ***实验分组的DAU至少10000以上*** ，否则实验周期将会很长，或很难获得准确的数据结果。

## 场景

### UI及交互
UI展示：“9.9元包邮”和“白菜价”哪个CTR高。
交互逻辑：“搜索功能应该在左侧还是右侧”
### 产品功能
产品方案：商品列表每屏展示一列还是两列能提高成交。
发版上线：小流量验证，效果正向的话再全量。

### 运营活动
给用户发1张100元红包还是发10个10元红包好。

### 策略算法
基于内容的推荐算法（根据用户的历史记录推荐相似内容）、
基于协同过滤的推荐算法（根据有相似兴趣用户的行为推荐相关内容）、
基于关联规则的推荐算法（根据内容本身的相关性给用户推荐），通过不同的算法进行A/B Test,r找到提高用户使用黏性的最佳方案。

“如何证明千人千面的策略优于热门推荐”

## 流程

![](_pic/ABT-Workflow.png)

### 发现问题
分析现有版本的各项数据指标，如购买转化率等，比如当前购买转化率仅有1%，根据当前的数据分析、竞品调研、KPI或其它多项对比，发现购买转化率比较低。

### 提出假设
在发现了问题以及确定了要优化的目标后，便可以开始对当前版本提出优化的想法和假设。  
如：针对购买转化率比较低，假设优化商品详情页或是购买流程等可以提升购买转化率，并将假设实施到产品方案中；  
限于团队资源有限，无法把所有的想法全部去验证，这就需要根据优先极排期，选择最重要的改进方案去做A/B Test。  

***在做A/B实验时可以大胆假设，小心求证。***  

### 实验设计（设置分组+订阅指标）
提出假设并确定了优化的方案后，可以针对要做A/B Test的方案进行实验设计；  
通常一个实验至少设置两个实验分组，一个是对照组（即线上旧方案），一个是实验组（即新方案），可以根据实际情况设置多个实验组；  
在此需要注意的是：***每个分组需要保证只有一个变量。***

在设置好实验分组后就要订阅这次实验关注的指标了，建议一个实验至少包含1-5个核心指标和若干个辅助指标；  
并且核心指标中除了要关注正向指标，同时也要负向指标；  
如做一个发push 和不发push的实验，除了在关注对【购买转化率】这个正向指标外，还要关注一下【APP卸载率】相关的负向指标。  
一个实验并不是关注的指标越多越好，但是必须要有能够决策这个实验结果的指标。  

### 运行实验
实验分组设置和订阅指标完毕后，实验可以发布并运行上线了；实验上线后可以实时关注数据，并根据数据调整实验流量。  

### 得出结论
实验运行一段时间后，就可以根据实验数据以分析结果了；  
A/B test分析将显示两个版本之间是否存在统计性显著差异，所以在分析结论时不止要观注实验分组之前的差异性，  
还要关注置信度和置信区间等统计指标来检测差异的真实性及可信度。

## 误区
## 误区1：一次测多个变量
在做A/B实验时最容易犯的一个误区就是：  
为了节约时间和成本实验组中有多个变量，当A/B的结果数据产出时，无法结出具体是哪个变量导致的数据上升或是下降。

正确做法：
一组对照实验中只有一个变量，且只能以所要验证的因素为变量，其它条件都相同，这样便于排除其它条件干扰实验。

## 误区2：人群不同质

对于一些已经意识到数据先验重要性的企业来说，  
为了验证新版本对于用户使用真实影响，可能会选择将不同版本打包，分别投放到不同的应用市场，  
当发现其中某版本的数据表现的最好，就决定将该版本全量上线。  

更有甚者，会随机选取一部分用户（甚至是男性用户和女性用户）进行前期试用，根据数据反馈决定迭代版本。  
这都违背了A/B测试的科学流量分配的原则，很容易造成辛普森悖论  
（即某个条件下的两组数据，分别讨论时都会满足某种性质或趋势，可一旦合并起来考虑，却可能导致相反的结论）。

正确做法： 科学的进行流量分配，保证每个试验版本的用户特征相类似。

## 误区3：实验不同时

首先需要明确，这种做法不是真正意义上的A/B测试；  
犯这种错误，是因为惯性思维使得我们脑中通常会有这样一个认知：产品两个不同版本的用户一直都是同一批人，会有相同的需求场景以及行为倾向。  

显然，不同时间周期中的用户行为是不同的，那用户行为反应出的数据自然就会不同：  
双十一前和双十一后，用户的消费倾向会不同；  
入夏前和入夏后，用户的生活作息会不同；  
产品上个版本有强运营的活动，而这个版本没有，用户在产品上的注意力会发生变化...这种认知最大的漏洞就是时间 。

正确做法： 不同版本方案并行（同时）上线试验，尽可能的降低所有版本的测试环境差别。

## 误区4：实验的样本量不足

在A/B实验中，我们无法知道所有用户的行为（如点击率）的真正均值……  
所以必须通过抽样，抽取一部分具有代表性的用户来测试不同版本的效果（例如均值），从而基于抽样数据进行统计分析……  

举个例子，某食品行业的老板要调研全国十几亿人喜欢吃啥主食，然后市场调研人员随便拉了几百号人来一问，  
哦这些人大部分喜欢吃米饭，然后就对老板说全国人民偏好吃大米？？？一想就知道不对。  

那么做一个A/B实验倒底需要多少流量（样本量）呢？
如果流量（样本量）配置少了 ，怕随机会高，得不到可信的结论，或者需要很长的时间才能得到可信的结论；
如果流量（样本量）配置多了，怕对用户的干扰太多，担心带来一些负责的影响。  

正确做法： 通过样本量计算器，根据业务关注的指标及当前实际值，量身定制计算出实验所需要的最小样本量。  

## 误区5：太早下结论

在你的A/B Test开始后，你可以观察增加每个测试变量的数据变化过程。  
它可以让你几乎实时看到哪些变量效果更好，并在看到转化率的基本差异是可以接受时，就选择效果较好的方案并中止实验。
 
正确做法： 大多数A/B Test在实验启动后开始向你展示指标数据的变化。  
但是，你不应该忘记在A/B测试实验中需要考虑的统计显着性，以获得可靠的结果。  
你应该让A/B测试运行一定的时间，并通过一定数量的访客，让它为你提供足够的相关数据，才能指导你做出更好的决策。  
如果你在进程的中间就停下来，你可能会得到一个不准确的统计数据。
